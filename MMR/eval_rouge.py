import sys
#sys.path.append("/data/projects/node_ptr/tools/xmltodict_0.10.2")
import os
import re
from shutil import copyfile
import xmltodict
from nltk.tokenize import sent_tokenize, word_tokenize
import subprocess
import argparse
import numpy as np
sys.path.append("./summary_util")
from summary_utils import *

parser = argparse.ArgumentParser(description="Generate Summary")
parser.add_argument('-s', '--max_sim', type=float, default=0.5, help='max_sim')
args = parser.parse_args()


MAX_SIM = args.max_sim # MAX_SIM is same as default from parser args

M_SIM = 0.5


data_dir = "./duc2004/docs"
eval_dir = "./duc2004/models"

def get_ref_summ(data_dir, eval_dir, out_dir):
    ref_files = os.listdir(eval_dir)

    refs = {}
    doc_cluster = sorted(os.listdir(data_dir))
    for cluster in doc_cluster:
        refs[cluster] = [ref for ref in ref_files if cluster[:-1] in ref[:len(cluster)-1].lower()]

    for cluster,ref in refs.iteritems():
        #print cluster,len(ref),ref
        for r in ref:
            summ = open(os.path.join(eval_dir,r)).readlines()
            with open(os.path.join(out_dir,r+'.html'),'w') as out_file:
                out_file.write('<html>\n')
                out_file.write('<head>\n')
                out_file.write('<title>%s</title>\n' % (r))
                out_file.write('</head>\n')
                out_file.write('<body bgcolor="white">\n')

                for i,s in enumerate(summ):
                    out_file.write('<a name="%d">[%d]</a> <a href="#%d" id=%d>%s</a>\n' % (i+1,i+1,i+1,i+1,s.strip()))

                out_file.write('</html>')
    return refs

def get_system_summ(data_dir, out_dir, test_result,salience_result, lamb , mini):
    # Read test result generated by hierptrnet
    #temp = 1
    results = open(test_result).read()
    results = results.strip().split('\n\n')
    result_dict = {}

    sal = open(salience_result).read()
    sal = sal.strip().split('\n\n')
    sal_dict = {}
    temp_dict = {}
    #print sal

    #print results  # results contain lines
    for r in results:
        f,label,pred = r.strip().split('\n')
        #print f
        cluster = f.split()[0].split('/')[0]
        #print cluster
        id_list = pred.strip().split()
        id_list = [(int(idx.split('/')[0]),int(idx.split('/')[1])) for idx in id_list]
        result_dict[cluster] = [idx for idx in id_list if idx != (1,0)]
        #print result_dict

    for s in sal:
        f2,doc1,doc2,doc3,doc4,doc5,doc6,doc7,doc8,doc9,doc10 = s.strip().split('\n')
        x1=doc1.split()
        x2=doc2.split()
        x3=doc3.split()
        x4=doc4.split()
        x5=doc5.split()
        x6=doc6.split()
        x7=doc7.split()
        x8=doc8.split()
        x9=doc9.split()
        x10=doc10.split()

        sal_dict[f2] = [x1,x2,x3,x4,x5,x6,x7,x8,x9,x10]
        #print sal_dict
       # x = np.zeros((50,10,30)) # cluster v/s docs v/s sentences
       # x[0,0,0] = value1
        #sal_dict[f2] = [ dictionary of all documents]
       # temp_dict[doc1] = [y for y in x]
        #print temp_dict
        #print "\n"
        #print x

    #print sal_dict['d30001t']
    

    doc_cluster = sorted(os.listdir(data_dir))
    for cluster in doc_cluster: 

        files = sorted(os.listdir(os.path.join(data_dir,cluster)))

        sent_file = []
        for (i,f) in enumerate(files):
            lines = open(os.path.join(data_dir,cluster,f)).read()
            #print lines
            lines = lines.replace('&', '')
            #print data_dir+'/'+f
            doc = xmltodict.parse(lines)
            #print doc
            sents = doc['DOC']['TEXT']
            #print sents
            #print
            sents = sents.strip().replace('\n',' ')
            sents = sent_tokenize(sents)
            #print len(sents)

            sent = []
            for s in sents:
                s = s.strip()
                #print s
                sent.append(s)

            sent_file.append(sent)

        #print cluster, files, len(sent), sum(len(s) for s in sent_file)
        pred = result_dict[cluster]
        #print pred
        #continue

        with open(os.path.join(out_dir,cluster+'.gcn_salience.html'),'w') as out_file:
            out_file.write('<html>\n')
            out_file.write('<head>\n')
            out_file.write('<title>%s.%s</title>\n' % (cluster, 'gcn_salience'))
            out_file.write('</head>\n')
            out_file.write('<body bgcolor="white">\n')

            current_summary = []
            tokenized_current_summary = []
            for i,s in enumerate(pred):
                if s == (1,0):
                    break
                if s[1] == 0:
                    continue
                doc_id = s[0] - 1
                sent_id = s[1] - 1

             #   print doc_id
            #    print sent_id
            #    print "\n"
                #print s
               # print "---"
                #print len(sent_file[doc_id])
                sent = sent_file[doc_id][sent_id].strip()
                tokenized_sent = word_tokenize(sent)

                summary_temp = current_summary + [sent]
                tokenized_summary_temp = tokenized_current_summary + [tokenized_sent]

                


                if len(tokenized_sent)>55 or len(tokenized_sent)<10: #Eliminate sentences with words >55 or <10
                    continue
               # if is_repeat(tokenized_sent, tokenized_current_summary, max_sim=MAX_SIM): # Skip sentences with sim>MAX_SIM from current summary
                    #if temp==1:
                       # print tokenized_sent
                        #print tokenized_current_summary
                        #temp = temp -1
                    #print "repeat"
                #    continue
                if is_redundant(sal_dict, doc_id, sent_id, cluster, lamb ,mini, tokenized_sent, tokenized_current_summary): # Skip sentences with sim>MAX_SIM from current summary
                    #if temp==1:
                       # print tokenized_sent
                        #print tokenized_current_summary
                        #temp = temp -1
                    #print "repeat"
                    continue
                if sum(len(s) for s in(summary_temp)) <= 665: #Limit Size
                    #print sent
                    current_summary = summary_temp
                    tokenized_current_summary = tokenized_summary_temp

                    out_file.write('<a name="%d">[%d]</a> <a href="#%d" id=%d>%s</a>\n' % (i+1,i+1,i+1,i+1,sent))

            out_file.write('</html>') # End of writing system summary for a cluster

    return

def write_setting_file(data_dir, refs, setting_f):
    with open(setting_f,'w') as f:
        f.write('<ROUGE-EVAL version="1.5.5">\n')
        i = 0
        doc_cluster = sorted(os.listdir(data_dir))
        for cluster in doc_cluster:
            i = i + 1
            f.write('<EVAL ID="%d">\n' % (i))
            f.write('<PEER-ROOT>systems</PEER-ROOT>\n')
            f.write('<MODEL-ROOT>models</MODEL-ROOT>\n')
            f.write('<INPUT-FORMAT TYPE="SEE"></INPUT-FORMAT>\n')
            f.write('<PEERS>\n')
            f.write('<P ID="%s">%s.%s.html</P>\n' % ('gcn_salience', cluster, 'gcn_salience'))
            #f.write('<P ID="%s">%s.%s.html</P>\n' % ('greedy', cluster, 'greedy'))
            f.write('</PEERS>\n')
            f.write('<MODELS>\n')
            for (j,r) in enumerate(refs[cluster]):
                f.write('<M ID="ref.%d">%s.html</M>\n' % (j, r))
            f.write('</MODELS>\n')
            f.write('</EVAL>\n')
        f.write('</ROUGE-EVAL>\n')
    return

for lamb in [x * 0.1 for x in range(0, 2)]:
   for mini in [y * 0.01 for y in range(-60,-30)]:
        print "lambda = ",lamb," & ","mini = ",mini
        if not os.path.exists("models/"):
            os.mkdir("models/")
        if not os.path.exists("systems/"):
            os.mkdir("systems/")

        #print "Producing Reference Summary (Gold-Standard, hand-written)"
        out_dir = "models/"
        refs = get_ref_summ(data_dir,eval_dir,out_dir)

        #print "Producing System Summary"
        out_dir = "systems/"
        test_result = "ducmulti_train_gcn.result.test"
        salience_result = "ducmulti_train_gcn.result.test.estimate"
        get_system_summ(data_dir,out_dir,test_result, salience_result, lamb, mini)

        setting_f = 'settings.xml'
        write_setting_file(data_dir, refs, setting_f)

        # run rouge
        ROUGE_path = os.path.abspath('/home/ayush/Desktop/MMR/ROUGE-1.5.5.pl')
        data_path  = os.path.abspath('/home/ayush/Desktop/MMR/data')
        rouge_cmd  = ['perl', ROUGE_path, "-e", data_path, "-n", "4", "-m", "-x", "-c", "95", "-r", "1000", "-f", "A", "-p", "0.5", "-t", "0", "-a", setting_f]
        subprocess.call(rouge_cmd)
